{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dfa496",
   "metadata": {},
   "source": [
    "# Questions to Answer \n",
    "1. Is the data homogenous in each column?\n",
    "\n",
    "Yes\n",
    "\n",
    "2. How do you anticipate this data will be used by data analysts and scientists downstream?\n",
    "\n",
    "This could be used to search for patterns in storm development and damage in order to help understand trends and minimize future damages.\n",
    "\n",
    "3. Does your answer to the last question give you an indication of how you can store the data for optimal querying speed and storage file compression?\n",
    "\n",
    "Generally yes, I have a good idea of how I would like to store and compress the data based on the access requirements. \n",
    "\n",
    "4. What cleaning steps do you need to perform to make your dataset ready for consumption?\n",
    "\n",
    "The data has been cleaned prior to the point where i pick it up, but I may perform certain joins based on testing.\n",
    "\n",
    "5. What wrangling steps do you need to perform to enrich your dataset with additional information?\n",
    "\n",
    "No wrangling or enrichment required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff41a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Used:\n",
    "\n",
    "Joined the three tables(details,locations,fatalities) on the primary key column Event_ID using q/kdb+ to determine \n",
    "\n",
    "event_id is unique for each row in detail (53187 rows in latest file)\n",
    "\n",
    "every fatality has an event_id match in detail (799 rows in latest file)\n",
    "  those have repeated values, so the distinct subset is less (434 rows in latest file)\n",
    "some fatalities have an event_id match in location (149 rows in latest file)\n",
    "  those have repeated values, so the distinct subset is less (89 rows in latest file)\n",
    "\n",
    "every location has an event_id match in detail (44760 rows in latest file)\n",
    "  those have repeated values, so the distinct subset is less (25965 rows in latest file)\n",
    "some locations have an event_id match in fatality (299 rows in latest file)\n",
    "  those have repeated values, so the distinct subset is less (89 rows in latest file)\n",
    "    \n",
    "q script: \"clean data.q\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d139d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q script: \"clean data.q\"\\n\\\\c 10 3000\\n\\ndetfiles:asc hsym each `$\\'system \"ls | grep details\"\\nlocfiles:asc hsym each `$\\'system \"ls | grep locat\"\\nfatfiles:asc hsym each `$\\'system \"ls | grep fatal\"\\n\\ndet: (,/) {(51#\"*\";enlist \",\") 0:x} each detfiles\\nloc: (,/) {(11#\"*\";enlist \",\") 0:x} each locfiles\\nfat: (,/) {(11#\"*\";enlist \",\") 0:x} each fatfiles\\n\\n\\nupdate BEGIN_DATE:(BEGIN_YEARMONTH,\\'BEGIN_DAY) from `det where not 1=count each BEGIN_DAY\\nupdate BEGIN_DATE:(BEGIN_YEARMONTH,\\'(\"0\",\\'BEGIN_DAY)) from `det where 1=count each BEGIN_DAY\\nupdate END_DATE:(END_YEARMONTH,\\'END_DAY) from `det where not 1=count each END_DAY\\nupdate END_DATE:(END_YEARMONTH,\\'(\"0\",\\'END_DAY)) from `det where 1=count each END_DAY\\nupdate BEGIN_DATETIME:\"Z\"$((BEGIN_DATE),\\'(\" \",\\'-8#\\'BEGIN_DATE_TIME)) from `det\\nupdate END_DATETIME:\"Z\"$((END_DATE),\\'(\" \",\\'-8#\\'END_DATE_TIME)) from `det\\n\\nupdate \"D\"$10#\\'FATALITY_DATE from `fat\\n\\nupdate \"M\"$YEARMONTH from loc\\n\\nupdate \"I\"$EPISODE_ID,\"I\"$EVENT_ID,`$STATE,\"I\"$INJURIES_DIRECT,\"I\"$INJURIES_INDIRECT,\"I\"$DEATHS_DIRECT,\"I\"$DEATHS_INDIRECT,\"F\"$MAGNITUDE,`$MAGNITUDE_TYPE,\"I\"$CATEGORY,`$TOR_F_SCALE,\"F\"$TOR_LENGTH,\"F\"$TOR_WIDTH,\"F\"$BEGIN_LAT,\"F\"$BEGIN_LON,\"F\"$END_LAT,\"F\"$END_LON,`$DATA_SOURCE from `det\\nupdate \"I\"$EPISODE_ID,\"I\"$EVENT_ID,\"I\"$LOCATION_INDEX,\"F\"$RANGE,`$AZIMUTH,`$LOCATION,\"F\"$LATITUDE,\"F\"$LONGITUDE,\"I\"$LAT2,\"I\"$LON2 from `loc\\nupdate \"I\"$FATALITY_ID,\"I\"$EVENT_ID,`$FATALITY_TYPE,\"I\"$FATALITY_AGE,`$FATALITY_SEX from `fat\\n\\nfat:fat lj (`EVENT_ID xkey select EPISODE_ID,EVENT_ID from det)\\n\\nsdet: select BEGIN_DATETIME,END_DATETIME,EPISODE_ID,EVENT_ID,STATE,EVENT_TYPE,CZ_NAME,INJ_DIR:INJURIES_DIRECT,INJ_IND:INJURIES_INDIRECT,DEAD_DIR:DEATHS_DIRECT,DEAD_IND:DEATHS_INDIRECT,\\n  DAMAGE_PROPERTY,DAMAGE_CROPS,SOURCE,BEGIN_LAT,BEGIN_LON,END_LAT,END_LON,EPISODE_NARRATIVE,EVENT_NARRATIVE from det\\nsloc: select EPISODE_ID,EVENT_ID,LOCATION,LATITUDE,LONGITUDE from loc\\nsfat: select FATALITY_DATE,FATALITY_ID,EVENT_ID,EPISODE_ID,FATALITY_TYPE,FATALITY_AGE,FATALITY_SEX,FATALITY_LOCATION from fat\\n\\nallmatch:(sdet ij `EPISODE_ID xkey sloc) ij `EPISODE_ID xkey sfat\\nallfull:(sdet lj `EPISODE_ID xkey sloc) lj `EPISODE_ID xkey sfat\\n\\ncompmatch:?[`allmatch;not,\\'enlist each @[(null;0=count\\');\"C\"=exec t from meta allmatch],\\'cols allmatch;0b;()]\\ncompmatch:`BEGIN_DATETIME`EPISODE_ID`EVENT_ID xdesc compmatch\\n\\nsave `:compmatch.csv\\n\\nrandeps:desc 10?count select i from compmatch where i = (min;i) fby EPISODE_ID\\nrandfull:select from compmatch where i in randeps\\nsave `:randfull.csv\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"q script: \"clean data.q\"\n",
    "\\c 10 3000\n",
    "\n",
    "detfiles:asc hsym each `$'system \"ls | grep details\"\n",
    "locfiles:asc hsym each `$'system \"ls | grep locat\"\n",
    "fatfiles:asc hsym each `$'system \"ls | grep fatal\"\n",
    "\n",
    "det: (,/) {(51#\"*\";enlist \",\") 0:x} each detfiles\n",
    "loc: (,/) {(11#\"*\";enlist \",\") 0:x} each locfiles\n",
    "fat: (,/) {(11#\"*\";enlist \",\") 0:x} each fatfiles\n",
    "\n",
    "\n",
    "update BEGIN_DATE:(BEGIN_YEARMONTH,'BEGIN_DAY) from `det where not 1=count each BEGIN_DAY\n",
    "update BEGIN_DATE:(BEGIN_YEARMONTH,'(\"0\",'BEGIN_DAY)) from `det where 1=count each BEGIN_DAY\n",
    "update END_DATE:(END_YEARMONTH,'END_DAY) from `det where not 1=count each END_DAY\n",
    "update END_DATE:(END_YEARMONTH,'(\"0\",'END_DAY)) from `det where 1=count each END_DAY\n",
    "update BEGIN_DATETIME:\"Z\"$((BEGIN_DATE),'(\" \",'-8#'BEGIN_DATE_TIME)) from `det\n",
    "update END_DATETIME:\"Z\"$((END_DATE),'(\" \",'-8#'END_DATE_TIME)) from `det\n",
    "\n",
    "update \"D\"$10#'FATALITY_DATE from `fat\n",
    "\n",
    "update \"M\"$YEARMONTH from loc\n",
    "\n",
    "update \"I\"$EPISODE_ID,\"I\"$EVENT_ID,`$STATE,\"I\"$INJURIES_DIRECT,\"I\"$INJURIES_INDIRECT,\"I\"$DEATHS_DIRECT,\"I\"$DEATHS_INDIRECT,\"F\"$MAGNITUDE,`$MAGNITUDE_TYPE,\"I\"$CATEGORY,`$TOR_F_SCALE,\"F\"$TOR_LENGTH,\"F\"$TOR_WIDTH,\"F\"$BEGIN_LAT,\"F\"$BEGIN_LON,\"F\"$END_LAT,\"F\"$END_LON,`$DATA_SOURCE from `det\n",
    "update \"I\"$EPISODE_ID,\"I\"$EVENT_ID,\"I\"$LOCATION_INDEX,\"F\"$RANGE,`$AZIMUTH,`$LOCATION,\"F\"$LATITUDE,\"F\"$LONGITUDE,\"I\"$LAT2,\"I\"$LON2 from `loc\n",
    "update \"I\"$FATALITY_ID,\"I\"$EVENT_ID,`$FATALITY_TYPE,\"I\"$FATALITY_AGE,`$FATALITY_SEX from `fat\n",
    "\n",
    "fat:fat lj (`EVENT_ID xkey select EPISODE_ID,EVENT_ID from det)\n",
    "\n",
    "sdet: select BEGIN_DATETIME,END_DATETIME,EPISODE_ID,EVENT_ID,STATE,EVENT_TYPE,CZ_NAME,INJ_DIR:INJURIES_DIRECT,INJ_IND:INJURIES_INDIRECT,DEAD_DIR:DEATHS_DIRECT,DEAD_IND:DEATHS_INDIRECT,\n",
    "  DAMAGE_PROPERTY,DAMAGE_CROPS,SOURCE,BEGIN_LAT,BEGIN_LON,END_LAT,END_LON,EPISODE_NARRATIVE,EVENT_NARRATIVE from det\n",
    "sloc: select EPISODE_ID,EVENT_ID,LOCATION,LATITUDE,LONGITUDE from loc\n",
    "sfat: select FATALITY_DATE,FATALITY_ID,EVENT_ID,EPISODE_ID,FATALITY_TYPE,FATALITY_AGE,FATALITY_SEX,FATALITY_LOCATION from fat\n",
    "\n",
    "allmatch:(sdet ij `EPISODE_ID xkey sloc) ij `EPISODE_ID xkey sfat\n",
    "allfull:(sdet lj `EPISODE_ID xkey sloc) lj `EPISODE_ID xkey sfat\n",
    "\n",
    "compmatch:?[`allmatch;not,'enlist each @[(null;0=count');\"C\"=exec t from meta allmatch],'cols allmatch;0b;()]\n",
    "compmatch:`BEGIN_DATETIME`EPISODE_ID`EVENT_ID xdesc compmatch\n",
    "\n",
    "save `:compmatch.csv\n",
    "\n",
    "randeps:desc 10?count select i from compmatch where i = (min;i) fby EPISODE_ID\n",
    "randfull:select from compmatch where i in randeps\n",
    "save `:randfull.csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c6649",
   "metadata": {},
   "source": [
    "### ERD Diagram in Slide Deck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd3c94",
   "metadata": {},
   "source": [
    "### Google Slide Deck: Exploratory_Data_Analysis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427653ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
