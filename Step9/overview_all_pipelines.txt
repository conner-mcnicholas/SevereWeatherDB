1) Initial Load Pipeline:2 parallel flows, manually triggered (init_pipeline)

			Activity 1 - Databricks Python 1 time batch load BOTH details and fatalities source to blob severeweathercontainer: (script_initial_files_to_blob.py)

			container:severeweathercontainer
				details x 23
				fatalities x 23

			Activity 2 - Copy details blob severeweathercontainer to MySQL
			Activity 3 - Copy fatalities blob severeweathercontainer to MySQL

			Pipeline Sequence: Activity 1 >> [Activity2, Activity 3] (chain)

			defaultdb.details + ~1M rows
			defaultdb.fatalities + ~20K rows

--------------------------------------------------------------------------------------------------------------------

2) Monthly Scheduled Pipeline: trigger on 17th of each month

			Activity 1 -  Databricks Python verify new filename for BOTH details and fatalities for latest year+month and send from source to blob newfiles container: (script_updated_file_to_blob.py)

			container:newfiles
				-details x 1
				-fatalities x 1

			---------

			Activity 2 - Copy updated details blob severeweathercontainer to MySQL Staging Table
			Activity 3 - Copy updated fatalities blob severeweathercontainer to MySQL Staging Table

			Activity 4 - Data Flow details upsert from MySQL Staging Table to MySQL Final Table
			Activity 5 - Data Flow details upsert from MySQL Staging Table to MySQL Final Table

			Activity 6 - Databricks Python cleanup container script, moving files from newfiles container to severeweathercontainer

			container:newfiles
				-details EMPTY
				-fatalities EMPTY

			----------

			Pipeline Sequence: Activity 1 >> [Activity2, Activity 3] >> [Activity4, Activity 5] >> Activity 6

			defaultdb.details + ~5k new rows
			defaultdb.fatalities + ~50 new rows

--------------------------------------------------------------------------------------------------------------------

3) Yearly Scheduled Pipeline: trigger on March 17th of each year

			Activity 1 - Databricks Python verify brand new source file for BOTH details and fatalities for latest year and send from source to newfiles blob container: (script_new_file_to_blob.py)

			container:newfiles
				-details x 1
				-fatalities x 1

			---------

			Activity 2 - Copy new details blob severeweathercontainer to MySQL Staging Table
			Activity 3 - Copy new fatalities blob severeweathercontainer to MySQL Staging Table

			Activity 4 - Data Flow details insert from MySQL Staging Table to MySQL Final Table
			Activity 5 - Data Flow details insert from MySQL Staging Table to MySQL Final Table

			Activity 6 - Databricks Python cleanup container script, moving files from newfiles container to severeweathercontainer

			container:newfiles
				-details EMPTY
				-fatalities EMPTY

			----------

		Pipeline Sequence: Activity 1 >> [Activity2, Activity 3] >> [Activity4, Activity 5] >> Activity 6

			defaultdb.details + ~5k new rows
			defaultdb.fatalities + ~50 new rows
