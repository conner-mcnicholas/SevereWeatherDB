{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.types import ArrayType, DoubleType, BooleanType\nfrom pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aec884e-21c0-4c7d-8827-1ea818f80438"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["schema_details = StructType([\n    StructField(\"BEGIN_YEARMONTH\", StringType(),True),\n    StructField(\"BEGIN_DAY\", StringType(),True),\n    StructField(\"BEGIN_TIME\", StringType(),True),\n    StructField(\"END_YEARMONTH\", StringType(),True),\n    StructField(\"END_DAY\", StringType(),True),\n    StructField(\"END_TIME\", StringType(),True),\n    StructField(\"EPISODE_ID\", IntegerType(),True),\n    StructField(\"EVENT_ID\", IntegerType(),True),\n    StructField(\"STATE\", StringType(),True),\n    StructField(\"STATE_FIPS\", IntegerType(),True),\n    StructField(\"EVENT_TYPE\", StringType(),True),\n    StructField(\"CZ_TYPE\", StringType(),True),\n    StructField(\"CZ_FIPS\", IntegerType(),True),\n    StructField(\"CZ_NAME\", StringType(),True),\n    StructField(\"WFO\", StringType(),True),\n    StructField(\"CZ_TIMEZONE\", StringType(),True),\n    StructField(\"INJURIES_DIRECT\", IntegerType(),True),\n    StructField(\"INJURIES_INDIRECT\", IntegerType(),True),\n    StructField(\"DEATHS_DIRECT\", IntegerType(),True),\n    StructField(\"DEATHS_INDIRECT\", IntegerType(),True),\n    StructField(\"DAMAGE_PROPERTY\", StringType(),True),\n    StructField(\"DAMAGE_CROPS\", StringType(),True),\n    StructField(\"SOURCE\", StringType(),True),\n    StructField(\"MAGNITUDE\", DoubleType(),True),\n    StructField(\"MAGNITUDE_TYPE\", StringType(),True),\n    StructField(\"FLOOD_CAUSE\", StringType(),True),\n    StructField(\"CATEGORY\", IntegerType(),True),\n    StructField(\"TOR_F_SCALE\", StringType(),True),\n    StructField(\"TOR_LENGTH\", DoubleType(),True),\n    StructField(\"TOR_WIDTH\", IntegerType(),True),\n    StructField(\"TOR_OTHER_WFO\", StringType(),True),\n    StructField(\"TOR_OTHER_CZ_STATE\", StringType(),True),\n    StructField(\"TOR_OTHER_CZ_FIPS\", IntegerType(),True),\n    StructField(\"TOR_OTHER_CZ_NAME\", StringType(),True),\n    StructField(\"BEGIN_RANGE\", IntegerType(),True),\n    StructField(\"BEGIN_AZIMUTH\", StringType(),True),\n    StructField(\"BEGIN_LOCATION\", StringType(),True),\n    StructField(\"END_RANGE\", IntegerType(),True),\n    StructField(\"END_AZIMUTH\", StringType(),True),\n    StructField(\"END_LOCATION\", StringType(),True),\n    StructField(\"BEGIN_LAT\", DoubleType(),True),\n    StructField(\"BEGIN_LON\", DoubleType(),True),\n    StructField(\"END_LAT\", DoubleType(),True),\n    StructField(\"END_LON\", DoubleType(),True),\n    StructField(\"EPISODE_NARRATIVE\",StringType(),True),\n    StructField(\"EVENT_NARRATIVE\",StringType(),True)\n])\n\nschema_fatalities = StructType([\n    StructField(\"FATALITY_ID INT\", IntegerType(),True),\n    StructField(\"FATALITY_ID\", IntegerType(),True),\n    StructField(\"EVENT_ID\", IntegerType(),True),\n    StructField(\"FATALITY_TYPE\", StringType(),True),\n    StructField(\"FATALITY_DATE\", StringType(),True),\n    StructField(\"FATALITY_AGE\", IntegerType(),True),\n    StructField(\"FATALITY_SEX\", StringType(),True),\n    StructField(\"FATALITY_LOCATION\",IntegerType(),True),\n    StructField(\"EVENT_YEARMONTH\",StringType(),True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ab87023-1caa-4605-81b0-5e9ed91381c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#2 fields below included in connection_string\nstorage_account_name = \"pipelinestorageacctaus\"\nstorage_account_access_key = \"f6fWRdrrX8qYB9a1y2Rlgu7qCuyeHuD59j3UIb0hi3ZanAn8DUmej+uofzFi7irJm954fTa5LtBb+AStzjJHYA==\"\nblob_container = \"pipelineauscontainer\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d116652f-c7b1-4ffe-aac0-8cdf4c8a607f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["detPath = \"wasbs://\"  + blob_container + \"@\" + storage_account_name + \".blob.core.windows.net/details/\"\nfatPath = \"wasbs://\"  + blob_container + \"@\" + storage_account_name + \".blob.core.windows.net/fatalities/\"\n\ndf_details = spark.read.format(\"csv\").option(\"header\", True).schema(schema_details).load(detPath)\ndf_fatalities = spark.read.format(\"csv\").option(\"header\", True).schema(schema_fatalities).load(fatPath)\n\nprint('count of details loaded to spark: ' +  str(df_details.count()))\nprint('count of fatalities loaded to spark: ' + str(df_fatalities.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"445af8c7-0808-4404-8261-cce64cd2ed73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">count of details loaded to spark: 320531320531320531320531320531320531320531320531320531320531\ncount of fatalities loaded to spark: 4513451345134513451345134513451345134513\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">count of details loaded to spark: 320531320531320531320531320531320531320531320531320531320531\ncount of fatalities loaded to spark: 4513451345134513451345134513451345134513\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nThis approach was dropped in favor of reading entire container folders directly to df\nRetaining code as it contains useful azure library utilities that may prove useful in later stages\n\"\"\"\n#from azure.storage.blob import BlobServiceClient\n#import re \n#The second command below was required in my databricks environment \n# pip install azure-storage-file\n# pip install azure-storage-blob\n'''\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=pipelinestorageacctaus;AccountKey=f6fWRdrrX8qYB9a1y2Rlgu7qCuyeHuD59j3UIb0hi3ZanAn8DUmej+uofzFi7irJm954fTa5LtBb+AStzjJHYA==;EndpointSuffix=core.windows.net\"\n\n# Instantiate a BlobServiceClient\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n# Instantiate a ContainerClient\npipeline_container_client = blob_service_client.get_container_client(blob_container)\n# Instantiate Files in Container\npipeline_container_list = pipeline_container_client.list_blobs()\n\n# Print list of all file in Container\nfor blobfile in pipeline_container_list:\n    print(blobfile.name)\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87571221-3660-4c2a-b8d0-61fdee1f05e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_pipeline_nbook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":39210620783688}},"nbformat":4,"nbformat_minor":0}
