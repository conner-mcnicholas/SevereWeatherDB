todo:
	install airflow image
	create dags
	figure out how to strip data to new rows using transform ADF data flow
		verify how to check tables for latest id and files for newest id

------------------------------------------------------------------------------------------------------------------------------------

1) Initial Load DAG:2 parallel flows: manually triggered

	Task 1 - PythonOperator 1 time batch load details source to blob severeweathercontainer: (batch_upload_to_blob.py)
	Task 2 - PythonOperator 1 time batch load fatalities source to blob severeweathercontainer  (batch_upload_to_blob.py)

	container:severeweathercontainer
		details x 23
		fatalities x 23

	Task 3 - AzureDataFactoryRunPipelineOperator 1 time ADF CP details blob severeweathercontainer to MySQL
	Task 4 - AzureDataFactoryRunPipelineOperator 1 time ADF CP fatalities blob severeweathercontainer to MySQL

	Task 1 >> Task 3
	Task 2 >> Task 4

	defaultdb.details + ~1M rows
	defaultdb.fatalities + ~20K rows

--------------------------------------------------------------------------------------------------------------------

2) Monthly Scheduled Trigger DAG: scheduled trigger on 17th of each month

	Task 1 - PythonOperator verify new details filename for latest year+month and send from source to blob newfiles container: (send_updated_file_to_blob.py)
	Task 2 - PythonOperator verify new fatalities filename for latest year+month and send from source to blob newfiles container:  (send_updated_file_to_blob.py)

	container:newfiles
		-details x 1
		-fatalities x 1

	---------

	Task 3 - AzureDataFactoryRunPipelineOperator single new file triggers ADF CP details blob newfiles container to MySQL (STRIP ROWS FOR ONLY NEW DATA)
	Task 4 - AzureDataFactoryRunPipelineOperator single new file triggers ADF CP fatalities blob newfiles container to MySQL (STRIP ROWS FOR ONLY NEW DATA)

	---------
	Task 5 - AzureDataFactoryRunPipelineOperator trigger ADF CP single details file from newfiles blob container to severeweathercontainer container
	Task 6 - AzureDataFactoryRunPipelineOperator trigger ADF CP single fatalities file from newfiles blob container to severeweathercontainer container

	container:newfiles
		-details EMPTY
		-fatalities EMPTY

	----------

	Task 1 >> Task 3 >> Task 5
	Task 2 >> Task 4 >> Task 6

	defaultdb.details + ~5k new rows
	defaultdb.fatalities + ~50 new rows

--------------------------------------------------------------------------------------------------------------------

3) Year Scheduled Trigger DAG: scheduled trigger on March 17th of each year

		Task 1 - PythonOperator verify brand new details source file for latest year and send from source to newfiles blob container: (send_new_file_to_blob.py)
		Task 2 - PythonOperator verify brand new fatalities source file for latest year and send from source to newfiles blob container:  (send_new_file_to_blob.py)

		container:newfiles
			-details x 1
			-fatalities x 1

		---------

		Task 3 - AzureDataFactoryRunPipelineOperator single new file triggers ADF CP details newfiles blob container to MySQL (VERIFY ONLY SEND NEW ROWS)
		Task 4 - AzureDataFactoryRunPipelineOperator single new file triggers ADF CP fatalities newfiles blob container to MySQL (VERIFY ONLY SEND NEW ROWS)

		---------
		Task 5 - AzureDataFactoryRunPipelineOperator trigger ADF CP single details file from newfiles blob container to severeweathercontainer container
		Task 6 - AzureDataFactoryRunPipelineOperator trigger ADF CP single fatalities file from newfiles blob container to severeweathercontainer container

		container:newfiles
			-details EMPTY
			-fatalities EMPTY

		----------

		Task 1 >> Task 3 >> Task 5
		Task 2 >> Task 4 >> Task 6

		defaultdb.details + ~5k new rows
		defaultdb.fatalities + ~50 new rows
